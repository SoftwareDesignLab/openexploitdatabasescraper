# Instructions

## High level overview of scrapers
The [scrapers](/exploit_scrapers/exploit_scrapers/spiders/) are made using [Scrapy](https://docs.scrapy.org/en/latest/). They take an initial URL and from there crawl through the website's pagination of exploits. On each page the scraper will attempt to gather all of the exploits and their data, saving the info into the [Exploit](/open_exploit_database_scraping/exploit/models.py) django model. Each scraper has two modes, an update mode and a full run mode. The update mode is used for just updating the database with a website's newer entries, while the full run mode crawls all the exploits the website has to offer. This is done to initially populate the database  with a website's exploits archive. To avoid detection, whenever the scrapy spiders load in the next page they does so using tor via a custom [downloader middleware](/exploit_scrapers/middlewares.py). Tor is also used when requesting an exploits example file during a full run, but not during an update run as tor is not needed then.

## Exploit model explanation
- What each scraper get at minimum
- Optional params as not all scrapers can get info

## Crawl command instructions
- Explain what the crawl command does
- How to use it
- Full synopsis on all of the params/flags
- How/where to add more spiders