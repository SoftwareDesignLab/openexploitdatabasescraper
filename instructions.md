# Instructions

## High level overview of scrapers
The [scrapers](/exploit_scrapers/exploit_scrapers/spiders/) are made using [Scrapy](https://docs.scrapy.org/en/latest/). They take an initial URL and from there crawl through the website's pagination of exploits. On each page the scraper will attempt to gather all of the exploits and their data, saving the info into the [Exploit](/open_exploit_database_scraping/exploit/models.py) django model. Each scraper has two modes, an update mode and a full run mode. The update mode is used for just updating the database with a website's newer entries, while the full run mode crawls all the exploits the website has to offer. This is done to initially populate the database  with a website's exploits archive. To avoid detection, whenever the scrapy spiders load in the next page they does so using tor via a custom [downloader middleware](/exploit_scrapers/middlewares.py). Tor is also used when requesting an exploits example file during a full run, but not during an update run as tor is not needed then.

## Exploit model explanation
To store an exploit and its data a django model is used. Aptly named "Exploit", [this](/open_exploit_database_scraping/exploit/models.py) model can store several attributes of a scraped exploit. Here are all of the model's fields and their descriptions:

- `source`: The source name of this exploit. Ex.: GitHub, CXSecurity, ...
- `name`: The name of the exploit.
- `is_repo`: Boolean field indicating if the exploit is a repo archive file.
- `date_published`: The datetime object of when the the exploit was published or uploaded, defaults to None.
- `date_created`: The datetime object of when the exploit model was created, autofills when created.
- `author`: The author or uploader of the exploit, defaults to None.
- `description`: A short description of the exploit, defaults to None.
- `download_failed`: Boolean value indicating if a failure to download the example exploit file occured, defaults to False.
- `example_file`: The exploit's example or demonstration file.

At a minimum each exploit model will have the source, name, is_repo, and date_created filled out. The other attributes are scraped when possible or have a default value.

## Crawl command instructions
The ability to run all of the scrapers has been made possible via the `crawl` command. [This](/open_exploit_database_scraping/exploit/management/commands/crawl.py) custom django command allows you to control which scrapers to run (or more accurately which scrapers not to run) and whether to do an update run or a full scrape run. A full synopsis of the `crawl` command's arguments can be viewed by typing `./manage.py crawl --help` or `python manage.py crawl --help`. 
NOTE: As this is a custom django command is must be run through [manage.py](/manage.py), this is why `./manage.py` precedes the `crawl` command as seen above.

Below are some use case examples of the handy dandy `crawl` command:
1. Let's say you want to run an update of the CXSecurity exploits, going back 2 days. Here's what the command would look like:  
`./manage.py crawl 2 --exploitdb --packetstorm --repo` or `./manage.py crawl 2 -e -p -r`  
Remember, the scraper flags (-e, -p, and -r) flag the scraper to NOT run as most of the time you'll want to update using all scrapers.  
Speaking of updating all of the sources...
2. If you want to update all of the sources going back 5 days, you would run:  
`./manage.py crawl 5` or if you wanted to scrape the entirety of all sources you would run `./manage.py crawl`. By not specifying the max number of days to go back the scrapers will simply scrape everything.
3. Some scrapers (such as CXSecurity) have the option to specify a start and end page for their source's exploit pagination:   `./manage.py crawl -s 10 -l 20` 
This command would result in CXSecurity and Packetstorm scraping pages 10-20, however the ExploitDB and Repo spiders do not make use of these parameters and would do a full run as `n_days` is not provided.  
In this case, it would be good to flag these two scrapers for no run: `./manage.py crawl -s 10 -l 20 -e -r`  
Now, only Packetstorm and CXSecurity will run using the pagination parameters handed to them.

## Integrating a new spider
Below are instructions on how to integrate a new Scrapy spider into the project. But first, it would be a good idea to familiarize yourself with how a [Scrapy spider](https://docs.scrapy.org/en/latest/intro/tutorial.html) works. It would also be a good idea to look into and re-use various logics within the current spiders. Such as the update delta, tor request for a full run, etc...  Now, to integrate a new spider into the project:
1. Start by adding your new spider to the same directory all the [other spiders](/exploit_scrapers/exploit_scrapers/spiders/) are located.
2. Once your spider is able to scrape all of the desired info, use the [save_exploit](/exploit_scrapers/exploit_scrapers/utils.py) helper function to save the scraped exploit info to the database as an exploit model. This function takes in the various attributes of the exploit model and assembles them into an exploit model, saving it to the database for you.
3. Lastly, in order for the `crawl` command to run your new spider, you'll need to add it to the [command's script](/open_exploit_database_scraping/exploit/management/commands/crawl.py). First, import your spider. Now, add a no-run flag for your scraper in `add_arguments()` And lastly, add you spider to `handle()` just like the other scrapers.