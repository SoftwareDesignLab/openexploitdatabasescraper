# Instructions

## High level overview of scrapers
The [scrapers](/exploit_scrapers/exploit_scrapers/spiders/) are made using [Scrapy](https://docs.scrapy.org/en/latest/). They take an initial URL and from there crawl through the website's pagination of exploits. On each page the scraper will attempt to gather all of the exploits and their data, saving the info into the [Exploit](/open_exploit_database_scraping/exploit/models.py) django model. Each scraper has two modes, an update mode and a full run mode. The update mode is used for just updating the database with a website's newer entries, while the full run mode crawls all the exploits the website has to offer. This is done to initially populate the database  with a website's exploits archive. To avoid detection, whenever the scrapy spiders load in the next page they does so using tor via a custom [downloader middleware](/exploit_scrapers/middlewares.py). Tor is also used when requesting an exploits example file during a full run, but not during an update run as tor is not needed then.

## Exploit model explanation
To store an exploit and its data a django model is used. Aptly named "Exploit", [this](/open_exploit_database_scraping/exploit/models.py) model can store several attributes of a scraped exploit. Here are all of the model's fields and their descriptions:

- `source`: The source name of this exploit. Ex.: GitHub, CXSecurity, ...
- `name`: The name of the exploit.
- `is_repo`: Boolean field indicating if the exploit is a repo archive file.
- `date_published`: The datetime object of when the the exploit was published or uploaded, defaults to None.
- `date_created`: The datetime object of when the exploit model was created, autofills when created.
- `author`: The author or uploader of the exploit, defaults to None.
- `description`: A short description of the exploit, defaults to None.
- `download_failed`: Boolean value indicating if a failure to download the example exploit file occured, defaults to False.
- `example_file`: The exploit's example or demonstration file.

At a minimum each exploit model will have the source, name, and is_repo filled out. The other attributes are scraped when possible or have a default value.

## Crawl command instructions
- Explain what the crawl command does
- How to use it
- Full synopsis on all of the params/flags
- How/where to add more spiders