from django.core.management.base import BaseCommand
from scrapy.crawler import CrawlerProcess
from django_cog import cog

# Import our spiders from our scrapy project.
from exploit_scrapers.exploit_scrapers.spiders import cxsecurity_spider, exploit_db_csv_spider, readme_spider, packetstorm_spider

class Command(BaseCommand):
    help = "Will scrape the various website for exploit info."

    def add_arguments(self, parser):
        # Add the optional arguement flag for updating.
        parser.add_argument("n_days", nargs='?', type=str, help="The max numbr of days to look back from the current date.", default=None)

    def handle(self, *args, **kwargs):
        # Init a crawler process for spiders.
        process = CrawlerProcess()
        # Scrapers go here.
        self.stdout.write("\n[INFO] Crawling CXSecurity...")
        process.crawl(cxsecurity_spider.CXSecuritySpider, kwargs["n_days"])
        self.stdout.write("\n[INFO] Done")
        self.stdout.write("\n[INFO] Crawling ExploitDB...")
        #process.crawl(exploit_db_csv_spider.CSVSpider, kwargs["n_days"])
        self.stdout.write("\n[INFO] Done")
        self.stdout.write("\n[INFO] Crawling Packet Storm...")
        #process.crawl(packetstorm_spider.PacketStormSpider, kwargs["n_days"])
        self.stdout.write("\n[INFO] Done")
        # Start the crawlers.
        process.start()
        # Once the are all crawled, stop the process.
        process.stop()
        self.stdout.write("\n[DEBUG] Crawling done.")

