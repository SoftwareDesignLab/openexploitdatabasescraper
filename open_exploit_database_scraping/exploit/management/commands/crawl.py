from django.core.management.base import BaseCommand
from scrapy.crawler import CrawlerProcess

# Import our spiders from our scrapy project.
from exploit_scrapers.exploit_scrapers.spiders import cxsecurity_spider, exploit_db_csv_spider, readme_spider, packetstorm_spider

class Command(BaseCommand):
    help = "Will scrape the various website for exploit info."

    def add_arguments(self, parser):
        # Add the optional arguement flag for updating.
        parser.add_argument("n_days", nargs='?', type=str, help="The number of days to go back from the current date to scrape for info.", default=None)

    def handle(self, *args, **kwargs):
        # Init a crawler process for spiders.
        process = CrawlerProcess()
        # Scrapers go here.
        #process.crawl(cxsecurity_spider.CXSecuritySpider, kwargs["n_days"])
        #process.crawl(exploit_db_csv_spider.CSVSpider, kwargs["n_days"])
        process.crawl(packetstorm_spider.PacketStormSpider, kwargs["n_days"])
        # Start the crawlers.
        process.start()
        process.stop()
        self.stdout.write("\n[DEBUG] Crawling done.")