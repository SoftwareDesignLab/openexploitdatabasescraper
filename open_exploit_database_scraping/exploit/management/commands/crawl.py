from django.core.management.base import BaseCommand
from scrapy.crawler import CrawlerProcess
from django_cog import cog

# Import our spiders from our scrapy project.
from exploit_scrapers.exploit_scrapers.spiders import cxsecurity_spider, exploit_db_csv_spider, readme_spider, packetstorm_spider

class Command(BaseCommand):
    help = "Will scrape the various website for exploit info."

    def add_arguments(self, parser):
        # Add the optional arguement flag for updating.
        parser.add_argument("n_days", nargs='?', type=str, help="The max numbr of days to look back from the current date.", default=None)
        # Add no-run flags for each scraper so that each scraper can be selectively turned off.
        parser.add_argument("-e", "--exploitdb", action="store_true", help="A no-run flag for the ExploitDB scraper, if passed the scraper will not run.")
        parser.add_argument("-c", "--cxsecurity", action="store_true", help="A no-run flag for the CXSecurity scraper, if passed the scraper will not run.")
        parser.add_argument("-p", "--packetstorm", action="store_true", help="A no-run flag for the PacketStorm scraper, if passed the scraper will not run.")


    def handle(self, *args, **kwargs):
        # Check if all of the scrapers have been marked to not run.
        if(kwargs["cxsecurity"] and kwargs["exploitdb"] and kwargs["packetstorm"]):
            self.stdout.write("\n[INFO] All crawlers flagged for no-run, exiting script.\n")
            return
        # Init a crawler process for spiders.
        process = CrawlerProcess()
        # Scrapers go here.
        if kwargs["cxsecurity"] is not True:
            process.crawl(cxsecurity_spider.CXSecuritySpider, kwargs["n_days"])

        if kwargs["exploitdb"] is not True:
            process.crawl(exploit_db_csv_spider.CSVSpider, kwargs["n_days"])
        
        if kwargs["packetstorm"] is not True:
            process.crawl(packetstorm_spider.PacketStormSpider, kwargs["n_days"])
        #Start the crawlers.
        self.stdout.write("\n[INFO] Crawler(s) starting...\n")
        process.start()
        # Once the are all crawled, stop the process.
        process.stop()
        self.stdout.write("\n[INFO] Crawling done.\n")

