# Instructions

## High level overview of scrapers
The [scrapers](/exploit_scrapers/exploit_scrapers/spiders/) are made using [Scrapy](https://docs.scrapy.org/en/latest/). They take an initial URL and from there crawl through the website's pagination of exploits. On each page the scraper will attempt to gather all of the exploits and their data, saving the info into the [Exploit](/open_exploit_database_scraping/exploit/models.py) django model. Each scraper has two modes, an update mode and a full run mode. The update mode is used for just updating the database with a website's newer entries, while the full run mode crawls all the exploits the website has to offer. This is done to initially populate the database  with a website's exploits archive. To avoid detection, whenever the scrapy spiders load in the next page they does so using tor via a custom [downloader middleware](/exploit_scrapers/middlewares.py). Tor is also used when requesting an exploits example file during a full run, but not during an update run as tor is not needed then.  
More details on each scraper can be found in the spiders' [README](/exploit_scrapers/exploit_scrapers/spiders/README.md).

## Exploit model explanation
To store an exploit and its data a django model is used. Aptly named "Exploit", [this](/open_exploit_database_scraping/exploit/models.py) model can store several attributes of a scraped exploit. Here are all of the model's fields and their descriptions:

- `source`: The source name of this exploit. Ex.: GitHub, CXSecurity, ...
- `name`: The name of the exploit.
- `is_repo`: Boolean field indicating if the exploit is a repo archive file.
- `date_published`: The datetime object of when the the exploit was published or uploaded, defaults to None.
- `date_created`: The datetime object of when the exploit model was created, autofills when created.
- `author`: The author or uploader of the exploit, defaults to None.
- `description`: A short description of the exploit, defaults to None.
- `download_failed`: Boolean value indicating if a failure to download the example exploit file occured, defaults to False.
- `example_file`: The exploit's example or demonstration file.

At a minimum each exploit model will have the source, name, is_repo, and date_created filled out. The other attributes are scraped when possible or have a default value.

## Crawl command instructions
The ability to run all of the scrapers has been made possible via the `crawl` command. [This](/open_exploit_database_scraping/exploit/management/commands/crawl.py) custom django command allows you to control which scrapers to run (or more accurately which scrapers not to run) and whether to do an update run or a full scrape run. A full synopsis of the `crawl` command's arguments can be viewed by typing `./manage.py crawl --help` or `python manage.py crawl --help`. 
NOTE: As this is a custom django command is must be run through [manage.py](/manage.py), this is why `./manage.py` precedes the `crawl` command as seen above.

Below are some use case examples of the handy dandy `crawl` command:
1. Let's say you want to run an update of the CXSecurity exploits, going back 2 days. Here's what the command would look like:  
`./manage.py crawl 2 --exploitdb --packetstorm --repo` or `./manage.py crawl 2 -e -p -r`  
Remember, the scraper flags (-e, -p, and -r) flag the scraper to NOT run as most of the time you'll want to update using all scrapers.  
Speaking of updating all of the sources...
2. If you want to update all of the sources going back 5 days, you would run:  
`./manage.py crawl 5` or if you wanted to scrape the entirety of all sources you would run `./manage.py crawl`. By not specifying the max number of days to go back the scrapers will simply scrape everything.
3. Some scrapers (such as CXSecurity) have the option to specify a start and end page for their source's exploit pagination:   `./manage.py crawl -s 10 -l 20` 
This command would result in CXSecurity and Packetstorm scraping pages 10-20, however the ExploitDB and Repo spiders do not make use of these parameters and would do a full run as `n_days` is not provided.  
In this case, it would be good to flag these two scrapers for no run: `./manage.py crawl -s 10 -l 20 -e -r`  
Now, only Packetstorm and CXSecurity will run using the pagination parameters handed to them.

## Integrating a new spider
Below are instructions on how to integrate a new Scrapy spider into the project. But first, it would be a good idea to familiarize yourself with how a [Scrapy spider](https://docs.scrapy.org/en/latest/intro/tutorial.html) works. It would also be a good idea to look into and re-use various logics within the current spiders. Such as the update delta, tor request for a full run, etc...  
Now, to integrate a new spider into the project:
1. Start by adding your new spider to the same directory all the [other spiders](/exploit_scrapers/exploit_scrapers/spiders/) are located.
2. Once your spider is able to scrape all of the desired info, use the [save_exploit](/exploit_scrapers/exploit_scrapers/utils.py) helper function to save the scraped exploit info to the database as an exploit model. This function takes in the various attributes of the exploit model and assembles them into an exploit model, saving it to the database for you.
3. Lastly, in order for the `crawl` command to run your new spider, you'll need to add it to the [command's script](/open_exploit_database_scraping/exploit/management/commands/crawl.py).  
First, import your spider.  
Now, add a no-run flag for your scraper in `add_arguments()`  
And lastly, add you spider to `handle()` just like the other scrapers.


## Creating and Exporting Database Snapshot
You'll need SSH access on the production server, and will need to first SSH into: `openexploit.crc.nd.edu`

Change directory to where the code is deployed: `/opt/docker/openexploitdatabasescraper`

Run the following command to create a new backup of the database:

```
docker compose -f production.yml exec postgres backup
```

This will create a new backup (let's call it `backup_2023_10_01T12_00_00.sql.gz`) in the container's `/backups` directory. 

Next, copy the new file from the docker container to your home directory:

```
docker cp $(docker compose -f production.yml ps -q postgres):/backups/backup_2023_10_01T12_00_00.sql.gz ~/
```

Once this is in your home directory, you can secure-copy it to your local machine (in a new terminal):

```
scp [USERNAME]@openexploit.crc.nd.edu:/home/[USERNAME]/backup_2023_10_01T12_00_00.sql.gz .
```

Finally, you can copy this to your local docker location and restore it.

**NOTE**: You'll need to make sure you only bring up the postgres service.  If Django has an open connection to the database, the restore could fail!

Bring up _only_ the postgres service with:

```
docker compose -f local.yml up postgres
```

Then copy the exported data into your local postgres's `/backups` folder with:

```
docker cp backup_2023_10_01T12_00_00.sql.gz $(docker compose -f production.yml ps -q postgres):/backups
```

And finally, restore it:

```
docker compose -f local.yml exec postgres restore backup_2023_10_01T12_00_00.sql.gz
```

## Accessing and Using the Exploit's API
There are two ways of accessing the API: through the browser or by requesting an authentication token and accessing the endpoint using that token.
### Accessing API through the browser
In order to access the Exploit's API endpoint via the browser, you just have to be logged in. Once logged in simply head to the `/api/exploits/` endpoint.
### Accessing the API Through a Token
If you're wanting to access the API through a script or through an API platform such as [Postman](https://www.postman.com/), this is the way to do it.  
First, you need to get an authentication token, to do this you must have an account.
To get an authentication token you'll need to make a POST request to `/api/authenticate/` with a data payload containing your login credentials in json format. E.g.,  
```
{
    "username": "<username_here>",
    "password": "<password_here>",
}
```
Provided that your credentials are valid, the response's content will contain your token formatted as:
```
{
    "token": "<token_key_here>",
}
```
Using your token, you can now set your authorization as follows:
```
{
    "Authorization": "Token <token_key_here>"
}
```
Once set, you will be able to access the `/api/exploits/` endpoint.
### Exploit Endpoint Parameters
There are three parameters you can make use of: `page`, `page_size`, and `cve_id`.  
By default `page=1`and `page_size=100`  
If you are accessing the endpoint through a browser you'll simply add these onto the url, here's a few examples:  
Let's say you wanted to access page three of the exploits where each page contains 10 entries:
 - `/api/exploits/?page=3&page_size=10`

Now let's say you only want exploits who's CVE ID contains the number 2, you would set the `cve_id` param to 2:
 - `/api/exploits/?cve_id=2`

If you are accessing the endpoint through a script you'll add your parameters into a json formatted data load. Here's a a couple examples:  
Here's the equivalent to the first example seen above:
```
{
    "page": 3,
    "page_size": 10,
}
```
And here's the equivalent to the second example seen above:
```
{
    "cve_id": 2,
}
```