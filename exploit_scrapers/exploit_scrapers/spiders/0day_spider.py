# Name: parse_0day.py
# Author: Mark Rumsey
# Date: 6/5/23
# Description: Downloads exploit data from 0day.today and saves into CSV file


# NOTE: This code has not yet been changed into a scrapy spider.


import csv
import re
import time
import random
from datetime import datetime
#from bs4 import BeautifulSoup
#import undetected_chromedriver as uc
#from selenium.webdriver.common.by import By
from selenium import webdriver
from selenium.webdriver.firefox.options import Options as FirefoxOptions

DAILY_REQS = 100                            # average requests made per day
MIN_DELAY = 86400*0.5/DAILY_REQS            # minimum delay between requests (0.5 * delay for average requests/day)
MAX_DELAY = 86400*1.5/DAILY_REQS            # maximum delay between requests (1.5 * delay for average requests/day)

# creates new driver
def new_driver():
    options = FirefoxOptions()
    options.add_argument('--headless')
    #options.binary_location = r'C:\Program Files\Mozilla Firefox\firefox.exe'
    driver = webdriver.Firefox(options=options)
    return driver

# agrees to terms
def agree(driver):
    try:
        agree_btn = driver.find_element(By.XPATH, '//input[@name="agree"]')
        agree_btn.click()
        time.sleep(10)
    except:
        return
    
# closes popup window
def close(driver):
    try:
        close_btn = driver.find_element(By.XPATH, '//a[@title="Close"]')
        close_btn.click()
    except:
        return

def search_category(name, writer):

    print(f'Searching {name} exploits')

    page_num = 1
    max_page = 0

    # loop though every page in category
    while True:

        # open next page
        driver = new_driver()
        driver.get(f'https://0day.today/{name}/{page_num}')
        time.sleep(15)

        # agree to terms and close popup
        agree(driver)
        close(driver)

        # parse html content
        contents = driver.page_source
        html = BeautifulSoup(contents, 'html.parser')
        table_contents = html.find_all('div', class_='ExploitTableContent')

        # get category and number of pages
        try:
            category = html.find('div', class_='category_title allow_tip_big').find('a')['href'][1:]
            max_page_link = html.find('div', class_='pages').find_all('a')[-1]['href']
        # try page again if HTML content does not load because of CAPTCHA
        except:
            print('Encountered CAPTCHA')
            driver.quit()
            time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))
            continue

        # update number of pages if changed
        new_max_page = int(re.search('/(\d*)$', max_page_link).group(1))
        if new_max_page > max_page:
            max_page = new_max_page

        print(f'Page {page_num} of {max_page}')

        # iterate through exploit table
        for exploit in table_contents:
            date, desc, id, platform, risk, cve, author = parse_table(exploit)
            writer.writerow([author, category, platform, risk, date, id, cve, desc])

        driver.quit()

        # update page number and wait for next request
        page_num += 1
        time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

        # end loop if last page
        if page_num > max_page:
            break

def parse_table(exploit):

    # get table cells
    data = exploit.find_all('div', class_=['td', 'td allow_tip'])

    date = data[0].text.strip()

    desc = data[1].find('h3').text.strip()

    # get 0day-id from link
    id = data[1].find('a')['href'].strip()
    id = re.search('\d+', id).group()

    platform = data[2].text.strip()

    # get risk level from string
    risk = data[4].text.strip()
    risk = re.search('\w+$', risk).group()

    # get CVE from tip string
    cve = data[7].find('div', class_='TipText').text.strip()
    if len(cve) != 0:
        cve = ' '.join(re.findall('CVE-\d*-\d*', cve))
    else:
        cve = 'N/A'

    author = data[10].find('a').text.strip()

    return date, desc, id, platform, risk, cve, author
    

def main():

    categories = ['remote', 'local', 'webapps', 'dos', 'shellcode']

    # create file with timestamp
    now = str(datetime.now()).replace(' ', '_')
    outfile = open(f'0day_exploits_{now}.csv', 'w')
    writer = csv.writer(outfile)

    # write header to csv
    header = ['Author', 'Category', 'Platform', 'Risk', 'Date Added (DD-MM-YYYY)', '0day-ID', 'CVE', 'Description']
    writer.writerow(header)

    # search all categories for exploits
    for category in categories:
        search_category(category, writer)

    outfile.close()


if __name__ == '__main__':
    main()