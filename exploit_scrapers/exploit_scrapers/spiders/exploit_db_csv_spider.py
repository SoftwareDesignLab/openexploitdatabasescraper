import sys
import requests
from pathlib import Path
from scrapy.spiders import CSVFeedSpider
from exploit_scrapers.exploit_scrapers.utils import save_exploit

class CSVSpider(CSVFeedSpider):
    name = "exploit_db_csv"
    # The starting (and only) URL is the raw csv file of the exploit db info.
    start_urls = ["https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"]
    delimiter = ","
    quotechar = "'"
    headers = ["id","file","description","date_published","author","type","platform","port","date_added","date_updated","verified","codes","tags","aliases","screenshot_url","application_url","source_url"]
    # Temporary counter to limit scraper to only scrape 10 files (as there are ~46k files).
    limiter = 0
    
    def parse_row(self, response, row):
        # Catch the header row
        if((row["id"] != "id") and (self.limiter < 10)): # And check if we are at our limit of scraped files.
            # Create django model here and when calling other spider, upload the file to the model.
            # Get the url of the example file of the current exploit.
            url = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/" + row["file"]
            # Use requests to download the example file.
            response = requests.get(url)
            # Save the file in exploits/ (Later will save file to a django model)
            exploit_file = url.split('/')[-1]
            filename = f"exploits/" + exploit_file
            Path(filename).write_bytes(response.content)
            # Increment our limiter
            self.limiter += 1
            print("\n[INFO] Limiter:",self.limiter)
        if(self.limiter > 10):
            sys.exit()
        
        