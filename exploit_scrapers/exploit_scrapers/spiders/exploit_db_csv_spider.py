import sys
import requests
from datetime import datetime, timedelta
from pathlib import Path
from scrapy.spiders import CSVFeedSpider
from exploit_scrapers.exploit_scrapers.utils import save_exploit


class CSVSpider(CSVFeedSpider):
    name = "exploit_db_csv"
    # The starting (and only) URL is the raw csv file of the exploit db info.
    start_urls = ["https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"]
    delimiter = ","
    quotechar = "'"
    headers = ["id","file","description","date_published","author","type","platform","port","date_added","date_updated","verified","codes","tags","aliases","screenshot_url","application_url","source_url"]

    def __init__(self, update=None):
        """When this spider is created we check if we're updating or if we're to scrape the whole site."""
        super(CSVSpider, self)
        self.update = update
    
    def parse_row(self, response, row):

        # Catch the header row
        if(row["id"] != "id"):
            if self.update:
                # Exploit DB date format is in MM/DD/YYYY
                date_pub = datetime.strptime(row["date_published"], "%m/%d/%Y")
            if(date_pub >= (datetime.now() - timedelta(days=self.update))):
                # I'm pretty sure it's supposed to be a greater than sign. Will test later.
                # For now just return as the CSV does not appear to be sorted by published date,
                # so we gotta check the whole thing.
                return
            # Get the url of the example file of the current exploit.
            url = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/" + row["file"]
            # Use requests to download the example file.
            response = requests.get(url)

            if response.content is not None:
                save_exploit(name=url.split('/')[-1], is_repo=False, file_name=url.split('/')[-1], file_content=response.content)
            else:
                save_exploit(name=url.split('/')[-1], is_repo=False)
        
        