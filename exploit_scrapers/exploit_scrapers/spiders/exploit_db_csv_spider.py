import sys
import environ
import requests
from time import sleep
from datetime import datetime, timedelta
import socket
from scrapy.spiders import CSVFeedSpider
from exploit_scrapers.exploit_scrapers.utils import save_exploit
from open_exploit_database_scraping.utils.tor_requests import RequestsTor

env = environ.Env()

class CSVSpider(CSVFeedSpider):
    source = "ExploitDB"
    name = "exploit_db_csv"
    # The starting (and only) URL is the raw csv file of the exploit db info.
    start_urls = ["https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"]
    delimiter = ","
    quotechar = "'"
    headers = ["id","file","description","date_published","author","type","platform","port","date_added","date_updated","verified","codes","tags","aliases","screenshot_url","application_url","source_url"]
    req_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101 Firefox/25.0",
        "From": "Center for Research Computing, Notre Dame, ebrinckm@gmail.com"
    }
    tor_request = None

    def __init__(self, update=None):
        """When this spider is created we check if we're updating or if we're to scrape the whole site."""
        super(CSVSpider, self)
        self.update = update
        tor_service_ip = socket.gethostbyname(env.str('TOR_HOST', default='tor'))
        self.tor_request = RequestsTor(tor_ports=(9050,), tor_cport=9051, host=tor_service_ip, autochange_id=5, password=env.str('TOR_CONTROLLER_PASSWORD', default='debug'))
    
    def parse_row(self, response, row):
        # Catch the header row
        if(row["id"] != "id"):
            # Exploit DB date format is in MM/DD/YYYY
            date_pub = datetime.strptime(row["date_published"], "%Y-%m-%d")
            if self.update:
                # If the date is older than the delta ...
                if(date_pub <= (datetime.now() - timedelta(days=int(self.update)))):
                    print(f"\n[DEBUG] Old entry encountered, skipping to next entry.")
                    # I'm pretty sure it's supposed to be a greater than sign. Will test later.
                    # For now just return as the CSV does not appear to be sorted by published date,
                    # so we gotta check the whole thing.
                    return
                print(f"\n[DEBUG] New entry encountered, saving.")
            # Get the url of the example file of the current exploit.
            url = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/" + row["file"]
            # Use requests to download the example file.
            if self.update:
                response = requests.get(url, headers=self.req_headers)
                # Sleep to deter detection.
                sleep(5.0)
            else:
                response = self.tor_request.get(url, headers=self.req_headers)
            # TODO: Save more information.
            if response.content is not None:
                save_exploit(source=self.source, name=url.split('/')[-1], is_repo=False, author=row["author"], date_published=date_pub, description=row["description"], file_name=url.split('/')[-1], file_content=response.content)
            else:
                save_exploit(source=self.source, name=url.split('/')[-1], is_repo=False, author=row["author"], date_published=date_pub, description=row["description"])
            # Sleep to deter detection
            sleep(3.0)
            
        
        