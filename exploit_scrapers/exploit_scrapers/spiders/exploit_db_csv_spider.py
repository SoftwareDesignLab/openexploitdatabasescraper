from scrapy.spiders import CSVFeedSpider
from scrapy.crawler import CrawlerProcess
from .exploit_db_examples_spider import ExamplesSpider


class CSVSpider(CSVFeedSpider):
    name = "exploit_db_csv"
    # The starting (and only) URL is the raw csv file of the exploit db info.
    start_urls = ["https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"]
    delimiter = ","
    quotechar = "'"
    headers = ["id","file","description","date_published","author","type","platform","port","date_added","date_updated","verified","codes","tags","aliases","screenshot_url","application_url","source_url"]
    # Temporary counter to limit scraper to only scrape 10 files (as there are ~46k files).
    limiter = 0
    # Init a crawler process so that our csv spider can employ the file scraper.
    process = CrawlerProcess()
    
    def parse_row(self, response, row):
        # Catch the header row
        if((row["id"] != "id") and (self.limiter < 10)): # And check if we are at our limit of scraped files.
            # Create django model here and when calling other spider, upload the file to the model.
            # Get the url of the example file of the current exploit.
            url = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/" + row["file"]
            # Employ the example file spider
            self.process.crawl(ExamplesSpider, url)
            # Increment our limiter
            self.limiter += 1
        elif self.limiter == -1:
            # if the limiter is -1 (we hit the file limit and have started the crawler) ignore the row.
            return
        else:
            # If we hit the file limit, start the crawlers and set the limit to -1 to signal to ignore further csv rows.
            self.process.start()
            self.limiter = -1
        
        