import sys
import requests
from pathlib import Path
from scrapy.spiders import CSVFeedSpider
from exploit_scrapers.exploit_scrapers.utils import save_exploit

class CSVSpider(CSVFeedSpider):
    name = "exploit_db_csv"
    # The starting (and only) URL is the raw csv file of the exploit db info.
    start_urls = ["https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"]
    delimiter = ","
    quotechar = "'"
    headers = ["id","file","description","date_published","author","type","platform","port","date_added","date_updated","verified","codes","tags","aliases","screenshot_url","application_url","source_url"]
    # Temporary counter to limit scraper.
    counter = 0
    limiter = 10

    def __init__(self, update=None):
        """When this spider is created we check if we're updating or if we're to scrape the whole site."""
        super(CSVSpider, self)
        self.update = update
    
    def parse_row(self, response, row):
        # Catch the header row
        if((row["id"] != "id") and (self.counter < self.limiter)): # And check if we are at our limit of scraped files.
            # Get the url of the example file of the current exploit.
            url = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/" + row["file"]
            # Use requests to download the example file.
            response = requests.get(url)

            if response.content is not None:
                save_exploit(name=url.split('/')[-1], is_repo=False, file_name=url.split('/')[-1], file_content=response.content)
            else:
                save_exploit(name=url.split('/')[-1], is_repo=False)

            # Increment our counter
            self.counter += 1
            print("\n[INFO] Limiter:",self.counter)
        if(self.counter > self.limiter):
            sys.exit()
        
        