import scrapy
import requests
from exploit_scrapers.exploit_scrapers.utils import save_exploit

class PacketStormSpider(scrapy.Spider):
    name = "packetstorm"
    start_urls = ["https://packetstormsecurity.com/files/tags/exploit/"]
    base_path = "https://packetstormsecurity.com"

    def parse(self, response):
        # Grab the info we want, do note that we need to handle the first entry as its own 
        # case as the class name is 'file first' and not 'file'. That is why we are 'adding'
        # two results together.
        exploits = response.xpath("//dl[@class='file']/dt/a/@href").getall() + response.xpath("//dl[@class='file first']/dt/a/@href").getall()
        titles = response.xpath("//dl[@class='file']/dt/a/text()").getall() + response.xpath("//dl[@class='file first']/dt/a/text()").getall()
        publish_dates = response.xpath("//dl[@class='file']/dd[@class='datetime']/a/text()").getall() + response.xpath("//dl[@class='file first']/dd[@class='datetime']/a/text()").getall()
        authors = response.xpath("//dl[@class='file']/dd[@class='refer']/a/text()").getall() + response.xpath("//dl[@class='file first']/dd[@class='refer']/a/text()").getall()
        descriptions = response.xpath("//dl[@class='file']/dd[@class='detail']/p/text()").getall() + response.xpath("//dl[@class='file first']/dd[@class='detail']/p/text()").getall()       
        # Iterate through exploits and grab file links. Here (but for now keep it handicapped)
        code = self.get_exploit_example(exploits[0])
        if code is not None:
            save_exploit(name=titles[0], is_repo=False, author=authors[0], description=descriptions[0], file_name=titles[0] + ".txt", file_content=code)
        else:
            save_exploit(name=titles[0], is_repo=False, author=authors[0], description=descriptions[0])
        
    def get_exploit_example(self, relative_path):
        """Helper function to get the example exploit code."""
        # Use requests to get the page containing the code.
        response = requests.get(self.base_path + relative_path)
        # Use scrapy's selector to get the code.
        html_selector = scrapy.Selector(response=response)
        return html_selector.xpath("//div[@class='src']/pre/code/text()").get()