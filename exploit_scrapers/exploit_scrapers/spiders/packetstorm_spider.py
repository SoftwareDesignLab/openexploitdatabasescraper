import scrapy
import requests
from time import sleep
from datetime import datetime, timedelta
from exploit_scrapers.exploit_scrapers.utils import save_exploit

class PacketStormSpider(scrapy.Spider):
    name = "packetstorm"
    start_urls = ["https://packetstormsecurity.com/files/tags/exploit/"]
    base_path = "https://packetstormsecurity.com"
    page = 1

    def __init__(self, update=None):
        """When this spider is created we check if we're updating or if we're to scrape the whole site."""
        super(PacketStormSpider, self)
        self.update = update

    def parse(self, response):
        # Grab the info we want, do note that we need to handle the first entry as its own 
        # case as the class name is 'file first' and not 'file'. That is why we are 'adding'
        # two xpath results together.
        exploits = response.xpath("//dl[@class='file']/dt/a/@href").getall() + response.xpath("//dl[@class='file first']/dt/a/@href").getall()
        titles = response.xpath("//dl[@class='file']/dt/a/text()").getall() + response.xpath("//dl[@class='file first']/dt/a/text()").getall()
        publish_dates = response.xpath("//dl[@class='file']/dd[@class='datetime']/a/text()").getall() + response.xpath("//dl[@class='file first']/dd[@class='datetime']/a/text()").getall()
        authors = response.xpath("//dl[@class='file']/dd[@class='refer']/a/text()").getall() + response.xpath("//dl[@class='file first']/dd[@class='refer']/a/text()").getall()
        descriptions = response.xpath("//dl[@class='file']/dd[@class='detail']/p/text()").getall() + response.xpath("//dl[@class='file first']/dd[@class='detail']/p/text()").getall()       
        # Iterate over all lists at the same time and create our exploits.
        # NOTE: This may be a bad idea if the lists are not the same size or out of alignment for whatever reason.
        for (exploit, title, date, author, description) in zip(exploits, titles, publish_dates, authors, descriptions):
            date_pub = datetime.strptime(date, "%b %d, %Y")
            if(self.update and (date_pub >= (datetime.now() - timedelta(days=self.update)))):  
                code = self.get_exploit_example(exploit)
                # Not all exploits have example code.
                if code is not None:
                    save_exploit(name=title, is_repo=False, author=author, description=description, file_name=title + ".txt", file_content=code)
                else:
                    save_exploit(name=title, is_repo=False, author=author, description=description)
            else:
                # The wesbite posts the exploits newest first so we can end the scraper here.
                print(f"\n[INFO] Packet Storm: Update limit of {self.update} days reached.")
                return
        # Now that we have processed the page, increment our page.
        self.page += 1
        # The html contains the max page number, so we get it.
        max_pages = int(response.xpath("//div[@id='nv']/form/fieldset/input[@id='page-max']/@value").get())
        print(f"\n[INFO] Packet Storm: Page {self.page} of {max_pages} crawled.")
        # Sleep to deter detection.
        sleep(0.25)
        if(self.page <= max_pages):
            # Construct the link to the next page: "https://packetstormsecurity.com/files/tags/exploit/pageN/"
            yield scrapy.Request(url=self.start_urls[0] + "page" + str(self.page) + '/')
        
    def get_exploit_example(self, relative_path):
        """Helper function to get the example exploit code."""
        # Use requests to get the page containing the code.
        response = requests.get(self.base_path + relative_path)
        # Use scrapy's selector to get the code.
        html_selector = scrapy.Selector(response=response)
        return html_selector.xpath("//div[@class='src']/pre/code/text()").get()