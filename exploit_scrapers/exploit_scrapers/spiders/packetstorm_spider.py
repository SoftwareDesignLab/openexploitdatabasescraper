import scrapy
import socket
import environ
import requests
from time import sleep
from random import choice
from datetime import datetime, timedelta
from exploit_scrapers.exploit_scrapers.utils import save_exploit
from open_exploit_database_scraping.utils.tor_requests import RequestsTor

env = environ.Env()

class PacketStormSpider(scrapy.Spider):
    source = "PacketStorm"
    name = "packetstorm"

    start_urls = ["https://packetstormsecurity.com/files/tags/exploit/"]
    base_page_url = "https://packetstormsecurity.com/files/tags/exploit/"
    base_path = "https://packetstormsecurity.com"
    req_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101 Firefox/25.0",
        "From": "Center for Research Computing, Notre Dame, ebrinckm@gmail.com"
    }
    user_agent_list = [
        "Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0",
        "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101 Firefox/25.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0",
    ]
    page = 1
    end_page = None
    tor_request = None

    def __init__(self, save_all, update=None, start_page=1, end_page=None):
        """When this spider is created we check if we're updating or if we're to scrape the whole site."""
        super(PacketStormSpider, self)
        if start_page is not None:
            self.page = int(start_page)
        else:
            self.page = 1
        self.update = update
        if end_page is not None:
            self.end_page = int(end_page)
        else:
            self.end_page = None
        tor_service_ip = socket.gethostbyname(env.str('TOR_HOST', default='tor'))
        self.tor_request = RequestsTor(tor_ports=(9050,), tor_cport=9051, host=tor_service_ip, autochange_id=5, password=env.str('TOR_CONTROLLER_PASSWORD', default='debug'))
        # update the start URL with the start page
        self.start_urls[0] = self.start_urls[0] + "page" + str(self.page) + '/'
        self.save_all = save_all

    def parse(self, response):
        # The html contains the max page number, so we get it.
        max_pages = int(response.xpath("//div[@id='nv']/form/fieldset/input[@id='page-max']/@value").get())
        if self.end_page is None:
            self.end_page = int(max_pages)
        # Grab the info we want, do note that we need to handle the first entry as its own 
        # case as the class name is 'file first' and not 'file'. That is why we are 'adding'
        # two xpath results together.
        exploits = response.xpath("//dl[@class='file']/dt/a/@href").getall() + response.xpath("//dl[@class='file first']/dt/a/@href").getall()
        titles = response.xpath("//dl[@class='file']/dt/a/text()").getall() + response.xpath("//dl[@class='file first']/dt/a/text()").getall()
        publish_dates = response.xpath("//dl[@class='file']/dd[@class='datetime']/a/text()").getall() + response.xpath("//dl[@class='file first']/dd[@class='datetime']/a/text()").getall()
        authors = response.xpath("//dl[@class='file']/dd[@class='refer']").getall() + response.xpath("//dl[@class='file first']/dd[@class='refer']").getall()
        descriptions = response.xpath("//dl[@class='file']/dd[@class='detail']/p/text()").getall() + response.xpath("//dl[@class='file first']/dd[@class='detail']/p/text()").getall()
        # Check if all of the lists are of the same length, if they are not then that means an exploit had more than one entry
        # of an attribute (description, publish date, etc...) and needs to be handled like the authors attribute.
        if((len(exploits) + len(titles) + len(publish_dates) + len(authors) + len(descriptions)) != len(exploits)*5): # 5 being the number of scraped attributes
            print("\n[WARNING] The lists of attributes are not equal in length, this will cause misalignment when saving the exploits to the database.\n\tOr in otherwords, bad data.\n")
            print(f"\n[INFO] Lengths of attributes scrapes:\nExploits: {len(exploits)}\nTitles: {len(titles)}\nPublished dates: {len(publish_dates)}\nAuthors: {len(authors)}\nDescriptions: {len(descriptions)}")
            print("\n[INFO] Skipping page.\n")
            self.page = self.page + 1

            if int(self.page) < int(self.end_page):
                yield scrapy.Request(url=self.base_page_url + "page" + str(self.page) + '/')
        # Iterate over all lists at the same time and create our exploits.
        # NOTE: This may be a bad idea if the lists are not the same size or out of alignment for whatever reason.
        for (exploit, title, date, author_html, description) in zip(exploits, titles, publish_dates, authors, descriptions):
            if self.update:
                date_pub = datetime.strptime(date, "%b %d, %Y")
                # If the exploit is not older than the cutoff delta, process it.
                if((date_pub >= (datetime.now() - timedelta(days=int(self.update))))):  
                    code, source_url, cve_id = self.get_exploit_example(exploit)
                    # Sleep to deter detection.
                    sleep(0.25)
                    # Because a single exploit may have multiple refers (authors/cites) we scraped per exploit
                    # and now we will get all the refers for that exploit using scrapy's selector.
                    author = scrapy.Selector(text=author_html).xpath("//a/text()").getall()
                    author = ", ".join(author) # If there are several authors combine them into one string comma delimited.
                    # Not all exploits have example code.
                    if code is not None:
                        save_exploit(source=self.source, save_all=self.save_all, name=title, source_url=source_url, is_repo=False, author=author, date_published=date_pub, description=description, cve_id=cve_id, file_name=title + ".txt", file_content=code)
                    else:
                        save_exploit(source=self.source, save_all=self.save_all, name=title, source_url=source_url, is_repo=False, author=author, date_published=date_pub, description=description, cve_id=cve_id)
                else:
                    # The wesbite posts the exploits newest first so we can end the scraper here.
                    print(f"\n[INFO] Packet Storm: Update limit of {self.update} days reached.")
                    return
            else:
                date_pub = datetime.strptime(date, "%b %d, %Y")
                code, source_url, cve_id = self.get_exploit_example(exploit)
                # Because a single exploit may have multiple refers (authors/cites) we scraped per exploit
                # and now we will get all the refers for that exploit using scrapy's selector.
                author = scrapy.Selector(text=author_html).xpath("//a/text()")
                # Not all exploits have example code.
                if code is not None:
                    save_exploit(source=self.source, save_all=self.save_all, name=title, source_url=source_url, is_repo=False, author=author, date_published=date_pub, description=description, cve_id=cve_id, file_name=title + ".txt", file_content=code)
                else:
                    save_exploit(source=self.source, save_all=self.save_all, name=title, source_url=source_url, is_repo=False, author=author, date_published=date_pub, description=description, cve_id=cve_id)
        
        print(f"\n[INFO] Packet Storm: Page {self.page} of {max_pages} crawled.")
        # Now that we have processed the page, increment our page.
        self.page = self.page + 1
        # Sleep to deter detection.
        sleep(0.25)
        if int(self.page) < int(self.end_page):
            # Construct the link to the next page: "https://packetstormsecurity.com/files/tags/exploit/pageN/"
            yield scrapy.Request(url=self.base_page_url + "page" + str(self.page) + '/')
        
    def get_exploit_example(self, relative_path):
        """Helper function to get the example exploit code."""
        url = self.base_path + relative_path
        # Use requests to get the page containing the code.
        if self.update:
            self.req_headers["User-Agent"] = choice(self.user_agent_list)
            response = requests.get(url, headers=self.req_headers)
            # Sleep to deter detection.
            sleep(5.0)
        else:
            response = self.tor_request.get(url, headers=self.req_headers)
        # Create the Scrapy html selector.
        html_selector = scrapy.Selector(response=response)
        # Use scrapy's selector to get the code.
        code = html_selector.xpath("//div[@class='src']/pre/code/text()").get()
        # Get the cve id(s) of the exploit.
        cve_id = html_selector.xpath("//dd[@class='cve']/a/text()").getall()
        # There may be more than one cve id so join the list into a comma seperated format.
        cve_id = ", ".join(cve_id)
        # Return code and its source.
        return (code, url, cve_id)