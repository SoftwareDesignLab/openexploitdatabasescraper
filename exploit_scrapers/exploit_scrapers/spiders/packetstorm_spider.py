import scrapy
import requests
from exploit_scrapers.exploit_scrapers.utils import save_exploit

class PacketStormSpider(scrapy.Spider):
    name = "packetstorm"
    start_urls = ["https://packetstormsecurity.com/files/tags/exploit/"]
    base_path = "https://packetstormsecurity.com"

    def parse(self, response):
        exploits = []
        exploits = exploits + response.xpath("//dl[@class='file']/dt/a/@href").getall() + response.xpath("//dl[@class='file first']/dt/a/@href").getall()
        # NOTE: Don't forget to get the first entry! (file first)
        # For now just keep these commented out
        titles = response.xpath("//dl[@class='file']/dt/a/text()").getall()
        publish_dates = response.xpath("//dl[@class='file']/dd[@class='datetime']/a/text()").getall()
        authors = response.xpath("//dl[@class='file']/dd[@class='refer']/a/text()").getall()
        descriptions = response.xpath("//dl[@class='file']/dd[@class='detail']/p/text()").getall()
        # Iterate through exploits and grab file links. Here:
        code = self.get_exploit_example(exploits[0])
        save_exploit(titles[0], is_repo=False, author=authors[0], description=descriptions[0])

    def get_exploit_example(self, relative_path):
        response = requests.get(self.base_path + relative_path)
        html_selector = scrapy.Selector(response=response)
        return html_selector.xpath("//div[@class='src']/pre/code/text()").getall()