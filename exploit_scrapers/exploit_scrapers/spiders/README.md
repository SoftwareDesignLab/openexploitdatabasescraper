# Detailed Spider Explanations
Each spider has the following in common:
- When a spider is run through the `crawl` command, it uses a custom downloader middleware that utilizes tor. This is done to prevent being blocked while traversing the website's pagination.
- Each spider makes use of a secondary request to obtain an exploit's example file.
- All scrapers use xpath via Scrapy's [Selectors](https://docs.scrapy.org/en/latest/topics/selectors.html) to obtain the desired data.
## CXSecurity Spider
The [CXSecurity spider](/exploit_scrapers/exploit_scrapers/spiders/cxsecurity_spider.py) can take in 3 arguments, the update delta, start page number, and endpage number.
- update: How many days to scrape back from the current date.
- start_page/end_page: What page in the website's pagination to start/end on.
This spider handles the update delta by checking the last date grouping on the current page, if that date is outside of the delta it stops scraping.
The spider is able to scrape several attributes for each exploit. It gathers:  
- the exploit's title
- the author(s)
- the example file (if available)
Scraping the example file is done differently than the other data gotten. A second request is made to get the file. This is done by the function `get_raw()`. Within this function you'll notice that this secondary request will only use tor if it is not running on an update delta. When not using tor it will use the [requests](https://pypi.org/project/requests/) package and sleep for 5 seconds after each request to deter detection.  
Unfortunately, due to the layout of the exploits and publish dates the spider does not scrape this info.
## PacketStorm Spider
The [PacketStorm spider](/exploit_scrapers/exploit_scrapers/spiders/packetstorm_spider.py) can take in 3 arguments, the update delta, start page number, and endpage number.
- update: How many days to scrape back from the current date.
- start_page/end_page: What page in the website's pagination to start/end on.
This spider is able to check each exploit's published date, and since the website sorts the exploits by newest first it stops as soon as it encounters an exploit that is outside of the update delta.
The spider is able to scrape several attributes for each exploit. It gathers:
- the exploit's title
- the author(s)
- the published date
- the exploits' description
- the example file (if available)
The `get_exploit_example()` function is used to get the expoit's example file, like CXSecurity it will only use tor if it is doing a full run rather than an update run. For the update run the spider will sleep for 5 seconds after each request to deter detection.
## ExploitDB Spider
The [ExploitDB spider](/exploit_scrapers/exploit_scrapers/spiders/exploit_db_csv_spider.py) only takes one parameter, update.
- update: How many days to scrape back from the current date.
The exploits within the CSV file are sorted newest first, so as soon as an exploit that is outide of the update delta is encoutnered, it stops.
This spider is different from the other spiders in that it only scrapes a CSV file using Scrapy's [CSVSpider](https://docs.scrapy.org/en/latest/topics/spiders.html#csvfeedspider). It is from this CSV file that we get the majority of an exploit's data. It gets:
- the exploit's title
- the author(s)
- the published date
- the exploits' description
- the example file (if available)
Like the other spiders it still uses a secondary request to retrieve an exploit's example file. This secondary request will only use tor if the spider is not on an update run.
## Repo Spider
The [Repo spider](/exploit_scrapers/exploit_scrapers/spiders/readme_spider.py) only takes on parameter, update.
- update: How many days to scrape back from the current date.
While the exploits within the readme file are sorted by year, they are not sorted beyond that. So, this spider handles the update delta by checking each exploit and only saving the ones that are within the delta and not already saved.  
The spider is scrapes the following:
- the exploit's title
- the author(s)
- the published date
- the example file (if available)
Like the ExploitDB spider, the repo spider only scrapes one document, a readme file from github containing information on several thousand exploit repos.  
This spider also makes use of a secondary request that uses tor regardless if it's on an update run or not.