from open_exploit_database_scraping.utils.tor_requests import RequestsTor
from exploit_scrapers.exploit_scrapers.utils import save_exploit
from datetime import datetime, timedelta
from time import sleep
import requests
import environ
import scrapy
import socket
import csv
import sys
import re

env = environ.Env()

class ReadMeSpider(scrapy.Spider):
    name = "readme"
    start_urls = ["https://raw.githubusercontent.com/nomi-sec/PoC-in-GitHub/master/README.md"]
    tor_request = None
    total = 0

    def __init__(self, update):
        self.update = update
        tor_service_ip = socket.gethostbyname(env.str('TOR_HOST', default='tor'))
        self.tor_request = RequestsTor(tor_ports=(9050,), tor_cport=9051, host=tor_service_ip, autochange_id=5, password=env.str('TOR_CONTROLLER_PASSWORD', default='debug'))

    def parse(self, response):
        
        for line in response.text.split('\n'):

            # parse file for CVE
            if line.startswith('###'):
                cve = re.search('CVE-\d*-\d*', line).group()
                # Get the publish date from the line. It's in a pair of parentheses
                date_pub= line.split('(')[-1].replace(')', '') # Clean out closing parentheses.
                # Convert to datetime object for django model.
                # NOTE: The assumption is made that all dates in the CVE line folloaw the YYYY-MM-DD format.
                try:
                    # Not all CVEs have a date, so we check (try/except) for that.
                    date_pub = datetime.strptime(date_pub, "%Y-%m-%d")
                except ValueError:
                    date_pub=None

            # parse file for urls
            if line.startswith('-'):
                if(self.update and (date_pub is None)):
                    # Skip exploit as we cannot tell if it is within our update delta.
                    continue
                # Check if the current exploit is within our update delta.
                if(self.update and (date_pub <= (datetime.now() - timedelta(days=int(self.update))))):
                    continue
                # Nab the repo link and author
                link = re.search('\(([^)]+)\)', line).group(1)
                author = link.split('/')[3]
                # Get the repo's archive file.
                zip_content = self.download_zip(link, cve+'_'+author+".zip")
                # Create the exploit model
                if zip_content is not None:
                    save_exploit(source="github", name=cve+'_'+author, is_repo=True, author=author, date_published=date_pub, file_name=cve+'_'+author+".zip", file_content=zip_content)
                else:
                    save_exploit(source="github", name=cve+'_'+author, is_repo=True, author=author, date_published=date_pub)
                self.total += 1
                print(f"[INFO] Total repos scraped: {self.total}")

    def download_zip(self, url, filename):
        """
        Helper function that attempts to download the repo's zip file.
        NOTE: This function operates off of the assumption that the main
              branch is named either 'main' or 'master'.
        """
        zip_urls = ("/archive/main.zip", "/archive/master.zip")
        not_found_code = 404
        # Attempt to download the archive file
        for zip_url in zip_urls:
            response = self.tor_request.get(url + zip_url)
            #response = requests.get(url + zip_url)
            sleep(5)
            # Check if the url was not valid
            if response.status_code == not_found_code:
                # Try the other archive url
                continue
            else:
                return response.content
        # If both archive urls fail, return None.
        return None
                