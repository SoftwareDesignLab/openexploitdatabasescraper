import scrapy
import requests
from time import sleep
from exploit_scrapers.exploit_scrapers.utils import save_exploit


class CXSecuritySpider(scrapy.Spider):
    name = "cxsecurity"
    start_urls = ["https://cxsecurity.com/exploit/"]
    req_headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101 Firefox/25.0",
        "From": "Center for Research Computing, Notre Dame, ebrinckm@gmail.com"
    }
    page = 1
    PAGES = 2#84
    total = 0

    def __init__(self, update=None):
        """When this spider is created we check if we're updating or if we're to scrape the whole site."""
        super(CXSecuritySpider, self)
        self.update = update

    def parse(self, response):
        # Get all links that go to an exploit from the page.
        exploit_links = response.xpath('//div[@class="col-md-7"]/h6/a/@href').getall()
        # Can also grab other attributes, but will keep them commented out for now
        titles = response.xpath('//div[@class="col-md-7"]/h6/a/@title').getall()
        authors = response.xpath('//div[@class="col-md-5"]/h6/span/a/text()').getall()
        self.page += 1
        self.total += len(exploit_links)
        # TODO: Save the additional info
        # Download the raw version of the exploit example. (Without employing another scraper)
        for (link, title, author) in zip(exploit_links, titles, authors):
            raw = self.get_raw(link) # For now only scrape the first raw example of each page.
            # Sleep to deter detection
            sleep(0.25)
            # Save the scraped info into a django model.
            exploit_name = link.split('/')[-1] # The id of the exploit file.
            # Not all exploits have an example file.
            if raw is not None:
                save_exploit(name=title, author=author, is_repo=False, file_name=exploit_name + ".txt", file_content=raw)
            else:
                save_exploit(name=title, author=author, is_repo=False)
        print(f"\n[INFO] CXSecurity: Page {self.page} of {self.PAGES} crawled.")

        # Now we check if the page contains exploit older than our update limit.
        date_pub = response.xpath("//thead/tr/th/center/u/h6/b/font/text()").getall()[-1] # Get the last (oldest) date.
        if(self.update and (date_pub >= (datetime.now() - timedelta(days=self.update)))):
            print(f"\n[INFO] CXSecurity: Update limit of {self.update} days reached.")
            return
        # Sleep to deter detection (ideally this would be somewhat random i.e. [0.25, 0.5]).
        sleep(0.25)
        if self.page < self.PAGES:
            yield scrapy.Request(url=self.start_urls[0] + str(self.page))

    def get_raw(self, url):
        """Helper function to get the exmaple exploit code."""
        # The raw version of the file is just the same URL with 'issue' replaced with 'ascii'
        # Example: https://cxsecurity.com/issue/WLB-2023060015 -> https://cxsecurity.com/ascii/WLB-2023060015
        # So replace 'issue' with 'ascii'. Python make this very easy.
        response = requests.get(url.replace("issue", "ascii"), headers=self.req_headers)
        # Unfortunately, CXSecurity's 'raw' files are in html and not exactly raw.
        # So we bring in a scrapy selector to get the example code within the html.
        html_selector = scrapy.Selector(response=response)
        # Now we will use the selector to get the code within the html document.
        return html_selector.xpath("//body/pre//text()").get()