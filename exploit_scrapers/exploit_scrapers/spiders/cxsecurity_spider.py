import scrapy
import requests
from random import choice
import socket
from time import sleep
from datetime import datetime, timedelta
from exploit_scrapers.exploit_scrapers.utils import save_exploit
from open_exploit_database_scraping.utils.tor_requests import RequestsTor


class CXSecuritySpider(scrapy.Spider):
    source = "CXSecurity"
    name = "cxsecurity"
    start_urls = ["https://cxsecurity.com/exploit/"]
    req_headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0",
        "From": "Center for Research Computing, Notre Dame, ebrinckm@gmail.com"
    }
    user_agent_list = [
        "Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0",
        "Mozilla/5.0 (Windows NT 6.1; WOW64; rv:25.0) Gecko/20100101 Firefox/25.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0",
    ]
    page = 1
    PAGES = 84
    counter = 0
    total = 0
    tor_request = None

    def __init__(self, update=None):
        """When this spider is created we check if we're updating or if we're to scrape the whole site."""
        super(CXSecuritySpider, self)
        self.update = update
        tor_service_ip = socket.gethostbyname('tor')
        self.tor_request = RequestsTor(tor_ports=(9050,), tor_cport=9051, host=tor_service_ip, autochange_id=5, password='5Z8Rigji8ymruhQOsjAs')

    def parse(self, response):
        # Get all links that go to an exploit from the page.
        exploit_links = response.xpath('//div[@class="col-md-7"]/h6/a/@href').getall()
        # Can also grab other attributes, but will keep them commented out for now
        titles = response.xpath('//div[@class="col-md-7"]/h6/a/@title').getall()
        authors = response.xpath('//div[@class="col-md-5"]/h6/span/a/text()').getall()
        if((len(exploit_links) + len(titles) +  len(authors)) != len(exploit_links)*3): # 3 being the number of scraped attributes
            print("\n[WARNING] The lists of attributes are not equal in length, this will cause misalignment when saving the exploits to the database.\n\tOr in otherwords, bad data.\n")
            print(f"\n[INFO] Lengths of attributes scrapes:\nExploits: {len(exploit_links)}\nTitles: {len(titles)}\nAuthors: {len(authors)}\n")
            print("\n[INFO] Skipping page.\n")
            if self.page < self.PAGES:
                self.page += 1
                yield scrapy.Request(url=self.start_urls[0] + str(self.page))
        
        # TODO: Save the additional info
        # Download the raw version of the exploit example. (Without employing another scraper)
        for (link, title, author) in zip(exploit_links, titles, authors):
            raw = self.get_raw(link) # For now only scrape the first raw example of each page.
            # if self.total % 100 == 0 and self.total > 0:
            #     print(f"{datetime.now()} === WAITING 5 MINUTES ===")
            #     sleep(300)
            #     self.counter = 0
            # # Sleep to deter detection
            # if self.counter >= 5:
            #     print(f"{datetime.now()} === WAITING 60 SECONDS ===")
            #     sleep(60)
            #     self.counter = 0
            # else:
            #     sleep(2.0)
            # sleep(1)
            
            # self.counter += 1
            # Save the scraped info into a django model.
            exploit_name = link.split('/')[-1] # The id of the exploit file.
            # Not all exploits have an example file.
            if raw is not None:
                pass#save_exploit(source=self.source, name=title, author=author, is_repo=False, file_name=exploit_name + ".txt", file_content=raw)
            else:
                pass#save_exploit(source=self.source, name=title, author=author, is_repo=False)
            self.total += 1
            print(f"== Gathered: {self.total}")
        print(f"\n[INFO] CXSecurity: Page {self.page} of {self.PAGES} crawled.")
        # Now we check if the page contains exploit older than our update limit.
        date_pub = response.xpath("//thead/tr/th/center/u/h6/b/font/text()").getall()[-1] # Get the last (oldest) date.
        # Convert the raw date into a datetime object
        date_pub = datetime.strptime(date_pub, "%Y-%m-%d")
        # Check if there are dates on page older than the cutoff delta.
        if(self.update and (date_pub <= (datetime.now() - timedelta(days=int(self.update))))):
            print(f"\n[INFO] CXSecurity: Update limit of {self.update} days reached.")
            return
        # Sleep to deter detection (ideally this would be somewhat random i.e. [0.25, 0.5]).
        # sleep(0.25)
        if self.page < self.PAGES:
            self.page += 1
            yield scrapy.Request(url=self.start_urls[0] + str(self.page))

    def get_raw(self, url):
        """Helper function to get the exmaple exploit code."""
        # The raw version of the file is just the same URL with 'issue' replaced with 'ascii'
        # Example: https://cxsecurity.com/issue/WLB-2023060015 -> https://cxsecurity.com/ascii/WLB-2023060015
        # So replace 'issue' with 'ascii'. Python make this very easy.
        # self.req_headers["User-Agent"] = self.user_agent_list[0] #choice(self.user_agent_list)
        response = self.tor_request.get(url.replace("issue", "ascii"), headers=self.req_headers)
        # Unfortunately, CXSecurity's 'raw' files are in html and not exactly raw.
        # So we bring in a scrapy selector to get the example code within the html.
        html_selector = scrapy.Selector(response=response)
        # Now we will use the selector to get the code within the html document.
        return html_selector.xpath("//body/pre//text()").get()