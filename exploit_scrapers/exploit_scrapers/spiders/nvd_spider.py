import os
import sys
import json
import scrapy
import socket
import environ
import requests
from io import BytesIO
from time import sleep
from zipfile import ZipFile
from urllib.request import urlopen
from datetime import datetime, timedelta
from exploit_scrapers.exploit_scrapers.utils import save_exploit
from open_exploit_database_scraping.utils.tor_requests import RequestsTor
from open_exploit_database_scraping.utils.process_html import run_from_script

env = environ.Env()

class NVDSpider(scrapy.Spider):
    source = "NVD"
    name = "nvd"
    start_urls = ["https://nvd.nist.gov/vuln/data-feeds"]
    base_url = "https://nvd.nist.gov"
    base_api_url = "https://services.nvd.nist.gov/rest/json/cves/2.0/"
    tor_request = None

    def __init__(self, save_all, update=None):
        self.update = update
        tor_service_ip = socket.gethostbyname(env.str('TOR_HOST', default='tor'))
        self.tor_request = RequestsTor(tor_ports=(9050,), tor_cport=9051, host=tor_service_ip, autochange_id=5, password=env.str('TOR_CONTROLLER_PASSWORD', default='debug'))

    def parse(self, response):
        if self.update:
            # Format dates like so for API call: [YYYY][“-”][MM][“-”][DD][“T”][HH][“:”][MM][“:”][SS][Z]
            now = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
            delta = (datetime.now() - timedelta(days=int(self.update))).strftime("%Y-%m-%dT%H:%M:%S")
            # Assemble the call. Add '.000' after each date to match NVD's API instructions.
            api_call = self.base_api_url + '?' + "pubStartDate=" + delta + ".000" + '&' + "pubEndDate=" + now + ".000"
            resp = requests.get(api_call, headers={"apiKey": "7b369e93-b1b4-4205-abd0-dc522b0f7590"})
            json_response = json.loads(resp.text)
            entries = self.parse_json(json_response, from_api=True)
            self.ship_off(entries)
            # Send assembled data to save function
            # vulnerabilities -> cve -> references
            return
        # Use xpath to get all of the links to CVE zipped json files
        zipped_json_links = response.xpath("//div[@id='divJSONFeeds']/div[@id='vuln-feed-table']/div/table[@class='xml-feed-table table table-striped table-condensed']/tbody/tr/td/a[contains(text(), 'ZIP')]/@href").getall()
        for link in zipped_json_links:
            if "modified" in link or "recent" in link:
                continue
            print(f"\n[INFO] Downloading {self.base_url + link}")
            # add relative link to the base url.
            nvd_json = self.load_zipped_json(self.base_url + link)
            entries = self.parse_json(nvd_json)
            self.ship_off(entries)
            # Handicap sys.exit below
            sys.exit()
            # -----------------------

    def load_zipped_json(self, url):
        json_string = ""
        response = urlopen(url)
        # Ensure the download was successful
        if response.status != 200:
            print("\n[ERROR] Zipped Download Failure")
            return
        zipped_json = ZipFile(BytesIO(response.read()))
        # Get the filename(s)
        filenames = zipped_json.namelist()
        # We are operating off the assumption that there is only 1 zipped json file.
        if len(filenames) != 1:
            print("\n[ERROR] Multiple zipped files")
            return
        # Open the zipped file and read and decode it into memory.
        for line in zipped_json.open(filenames[0]).readlines():
            json_string += line.decode("utf-8")
        # Load the read file into a json object.
        json_file = json.loads(json_string)
        return json_file

    def parse_json(self, json_file, from_api=False):
        # Parse the wanted info from the json.
        entries = []
        # CVE_Items -> cve -> references -> reference_data: url and name
        if from_api:
            # The API json response differs from the json feeds.
            cve_items = json_file["vulnerabilities"]
        else:
            cve_items = json_file["CVE_Items"]

        for cve in cve_items:
            # Get the reference data for each cve.
            if from_api:
                references = cve["cve"]["references"]
                name = cve["cve"]["id"] + ' ' + cve["cve"]["published"]
            else:
                references = cve["cve"]["references"]["reference_data"]

            for ref in references:
                url = ref["url"]

                if not from_api:
                    name = ref["name"]
                # If the reference is from a site we've already scraped, skip it.
                if("packetstorm" in url or "cxsecurity" in url or "exploit-db" in url):
                    continue
                try:
                    response = self.tor_request.get(url)
                    if response.status_code == 200:
                        html_content = response.content
                        entries.append((url, name, html_content))
                    else:
                        print("\n[ERROR] Non 200 Code")
                        continue
                except Exception as e:
                    print(f"\n[ERROR] {e}")   
                    continue
                # Handicap return below------
                return entries
                #----------------------------
        return entries

    def ship_off(self, entries):
        for entry in entries:
            meta_data = run_from_script(entry[-1], entry[0])
            print("-------------------------------------------")
            print(meta_data.keys())
            print("-------------------------------------------")
            # Handicap return
            return
            #----------------
            if meta_data:
                cve = meta_data["CVE"]
                vendor = meta_data["Vendor"]
                product = meta_data["Product"]
                vul_type = meta_data["Vulnerability_Type"]
                risk = meta_data["Risk"]
                pub_dates = meta_data["Published_dates"]
                version = meta_data["Version_Number"]
            # Save
            #save_exploit()

    