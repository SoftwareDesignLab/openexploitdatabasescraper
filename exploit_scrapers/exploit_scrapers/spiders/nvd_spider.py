import os
import json
import scrapy
import socket
import environ
import requests
from time import sleep
from urllib.request import urlopen
from datetime import datetime, timedelta
from exploit_scrapers.exploit_scrapers.utils import save_exploit
from open_exploit_database_scraping.utils.tor_requests import RequestsTor
#from open_exploit_database_scraping.utils.process_html import run_from_script

class NVDSpider(scrapy.Spider):
    source = "NVD"
    name = "nvd"
    start_urls = ["https://nvd.nist.gov/vuln/data-feeds"]
    base_url = "https://nvd.nist.gov"

    def __init__(self, save_all, update=None):
        self.update = update

    def parse(self, response):
        total = 0
        if update:
            # Use API here
            return
        # Use xpath to get all of the links to CVE zipped json files
        zipped_json_links = response.xpath("//div[@id='divJSONFeeds']/div[@id='vuln-feed-table']/div/table[@class='xml-feed-table table table-striped table-condensed']/tbody/tr/td/a[contains(text(), 'ZIP')]/@href").getall()
        for link in zipped_json_links:
            print(f"\n[INFO] Downloading {base_url + link}")
            # add relative link to the base url.
            nvd_json = load_zipped_json(base_url + link)
            total += parse_json(nvd_json)
        print(f"\n[INFO] Number of refs: {total}")
            

    def load_zipped_json(self, url):
        json_string = ""
        # TODO: Use tor here
        zipped_json = urlopen(url)
        # Ensure the download was successful
        if zipped_json.status != 200:
            print("\n[ERROR] Zipped Download Failure")
            return
        # Get the filename(s)
        filenames = zipped_json.namelist()
        # We are operating off the assumption that there is only 1 zipped json file.
        if len(filenames) != 1:
            print("\n[ERROR] Multiple zipped files")
            return
        # Open the zipped file and read and decode it into memory.
        for line in zipped_json.open(filenames[0]).readlines():
            json_string += line.decode("utf-8")
        # Load the read file into a json object.
        json_file = json.loads(json_string)
        return json_file

    def parse_json(self, json_file):
        # Parse the wanted info from the json.
        entries = []
        # CVE_Items -> cve -> references -> reference_data: url and name
        cve_items = json_file["CVE_Items"]
        for cve in cve_items:
            # Get the reference data for each cve.
            references = cve["cve"]["references"]["reference_data"]
            return len(references)
            for ref in references:
                # TODO: Check if it's a source we've already scraped.
                url = ref["url"]
                name = ref["name"]
                # TODO: replace with tor
                try:
                    response = requests.get(url)
                    if response.status_code == 200:
                        html_content = response.content
                        entries.append((url, name, html_content))
                    else:
                        print("\n[ERROR] Non 200 Code")
                        continue
                except Exception as e:
                    print(f"\n[ERROR] {e}")   
                    continue

    def ship_off(self, entries):
        for entry in entries:
            # Save html file
            pass
            #meta_data = run_from_script(entries[-1], entries[0])
        #os.system("python open_exploit_database_scraping/utils/process_html.py {} {} {}".format())

    