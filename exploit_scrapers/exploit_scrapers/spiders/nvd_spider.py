import json
import scrapy
import socket
import environ
import requests
from io import BytesIO
from time import sleep
from zipfile import ZipFile
from urllib.request import urlopen
from datetime import datetime, timedelta
from exploit_scrapers.exploit_scrapers.utils import save_exploit
from open_exploit_database_scraping.utils.tor_requests import RequestsTor
from open_exploit_database_scraping.utils.process_html import run_from_script

env = environ.Env()

class NVDSpider(scrapy.Spider):
    source = "NVD"
    name = "nvd"
    start_urls = ["https://nvd.nist.gov/vuln/data-feeds"]
    base_url = "https://nvd.nist.gov"
    base_api_url = "https://services.nvd.nist.gov/rest/json/cves/2.0/"
    tor_request = None

    def __init__(self, save_all, update=None):
        self.save_all = save_all
        self.update = update
        tor_service_ip = socket.gethostbyname(env.str('TOR_HOST', default='tor'))
        self.tor_request = RequestsTor(tor_ports=(9050,), tor_cport=9051, host=tor_service_ip, autochange_id=5, password=env.str('TOR_CONTROLLER_PASSWORD', default='debug'))

    def parse(self, response):
        # If we're doing an update run, call the API.
        if self.update:
            # Format dates like so for API call: [YYYY][“-”][MM][“-”][DD][“T”][HH][“:”][MM][“:”][SS][Z]
            now = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")
            delta = (datetime.now() - timedelta(days=int(self.update))).strftime("%Y-%m-%dT%H:%M:%S")
            # Assemble the call. Add '.000' after each date to match NVD's API instructions.
            api_call = self.base_api_url + '?' + "pubStartDate=" + delta + ".000" + '&' + "pubEndDate=" + now + ".000"
            # Add the API key to the request headers as the NVD API instructions state.
            resp = requests.get(api_call, headers={"apiKey": "7b369e93-b1b4-4205-abd0-dc522b0f7590"})
            # Load the json response into and a json object.
            json_response = json.loads(resp.text)
            # Parse the response for references.
            entries = self.parse_json(json_response, from_api=True)
            # Ship these references off to the ML pipeline.
            self.ship_off(entries)
            # We're done here.
            return
        # If we're not doing an update run, we will scrape the zipped json feeds.
        # Use xpath to get all of the links to CVE zipped json files
        zipped_json_links = response.xpath("//div[@id='divJSONFeeds']/div[@id='vuln-feed-table']/div/table[@class='xml-feed-table table table-striped table-condensed']/tbody/tr/td/a[contains(text(), 'ZIP')]/@href").getall()
        # Iterate through all of the zip links...
        for link in zipped_json_links:
            print(f"\n[INFO] Downloading {self.base_url + link}")
            # Load the json into memory.
            nvd_json = self.load_zipped_json(self.base_url + link)
            # Parse the json for its references.
            if nvd_json:
                entries = self.parse_json(nvd_json)
            # Ship off the references to the process_html.py script.
            self.ship_off(entries)

    def load_zipped_json(self, url):
        json_string = ""
        # Use urlopen to download the zipped json feed.
        response = urlopen(url)
        # Ensure the download was successful
        if response.status != 200:
            print("\n[ERROR] Zipped Download Failure")
            return
        # Use python's ZipFile to handle the zip file.
        zipped_json = ZipFile(BytesIO(response.read()))
        # Get the filename(s)
        filenames = zipped_json.namelist()
        # We are operating off the assumption that there is only 1 zipped json file.
        if len(filenames) != 1:
            print(f"\n[ERROR] Expected 1 zip file but got {len(filenames)}.")
            return
        # Open the zipped file, read, and decode it into memory.
        for line in zipped_json.open(filenames[0]).readlines():
            json_string += line.decode("utf-8")
        # Load the read file into a json object.
        json_file = json.loads(json_string)
        return json_file

    def parse_json(self, json_file, from_api=False):
        # Parse the wanted info from the json.
        entries = []
        # The API json response differs from the json feeds so we need to handle it differently.
        if from_api:
            # Vulnerabilities -> cve -> references: url
            cve_items = json_file["vulnerabilities"]
        else:
            # CVE_Items -> cve -> references -> reference_data: url and name
            cve_items = json_file["CVE_Items"]

        for cve in cve_items:
            # Get the reference data for each cve.
            if from_api:
                references = cve["cve"]["references"]
                # The API JSON does not have a name with each ref, so we make one.
                name = cve["cve"]["id"] + ' ' + cve["cve"]["published"]
            else:
                references = cve["cve"]["references"]["reference_data"]

            for ref in references:
                # Get each ref's url.
                url = ref["url"]
                # Get the name from the ref entry if it's from the JSON feeds.
                if not from_api:
                    name = ref["name"]
                # If the reference is from a site we've already scraped, skip it.
                if("packetstorm" in url or "cxsecurity" in url or "exploit-db" in url):
                    continue
                try:
                    # Use to to download each ref page.
                    response = self.tor_request.get(url)
                    # A lot of the links are dead, so make sure we get a good response code.
                    if response.status_code == 200:
                        html_content = response.content
                        # Assemble a tuple containing the wanted info
                        entries.append((url, name, html_content))
                    else:
                        print("\n[ERROR] Non 200 status code, skipping reference.")
                        continue
                except Exception as e:
                    print(f"\n[ERROR] {e}")   
                    continue
        # Return the list of entries where each entry is a tuple with the following format: (url, name, html_content)
        return entries

    def ship_off(self, entries):
        # Iterate through each entry and ship it off to the ML script.
        for entry in entries:
            meta_data = run_from_script(entry[-1], entry[0])
            
            if meta_data:
                # The name is in the middle. entry -> (url, name, html_content)
                name = entry[1]
                try:
                    # The desired info is in a list form, we want it as a string.
                    # So we join the list into a string.
                    cve = ''.join(meta_data["CVE"])
                    vendor = ''.join(meta_data["Vendor"])
                    product = ''.join(meta_data["Product"])
                    vul_type = ''.join(meta_data["Vulnerability_Type"])
                    risk = ''.join(meta_data["Risk"])
                    pub_dates = ''.join(meta_data["Published_dates"])
                    version = ''.join(meta_data["Version_Number"])
                    # Save the assembled data into an Exploit model.
                    save_exploit(
                        source=self.source, name=name, is_repo=False, save_all=self.save_all, cve_id=cve, vendor=vendor,
                        product=product, vul_type=vul_type, risk=risk, pub_dates=pub_dates, version=version
                        )
                except KeyError:
                    print("[ERROR] Given dictionary keys do not have all of the expected keys.")
                    print("Got:")
                    print("-------------------------------------------")
                    print(meta_data.keys())
                    print("-------------------------------------------")
                    print("Expected:")
                    print("-------------------------------------------")
                    print("CVE, Vendor, Product, Vulnerability_Type, Risk, Published_dates, and Version_Number")
                    print("-------------------------------------------")
                    continue

    